# -*- coding: utf-8 -*-
"""Ibitinga_Invernada-TeseFinalDoutorado(apenas LSTM e dados e Augumented Data).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UWxVPvMxGQA5MZQeJ5kCMdw5H6i7P8bH
"""

import os
# Configurações de paralelismo (antes de importar o TensorFlow)
os.environ['TF_DETERMINISTIC_OPS'] = '1'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # (Opcional) Reduz logs desnecessários

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import backend as K

from google.colab import drive
drive.mount('/content/drive')

# Configurações para controle estrito de execução
tf.config.threading.set_inter_op_parallelism_threads(1)
tf.config.threading.set_intra_op_parallelism_threads(1)

# Definir a seed para a reprodutibilidade
seed_value = 42
np.random.seed(seed_value)
tf.random.set_seed(seed_value)

# Limpar qualquer sessão do Keras anterior
K.clear_session()

# Carregar dados
file_path = '/content/drive/MyDrive/Unicamp/Doutorado/TeseFinal_Dados/IBITINGA INVERNADA/IBITINGA INVERNADA - Dados para processar Rede-CSV.csv'
data = pd.read_csv(file_path, delimiter=';')

data['Data Leitura'] = pd.to_datetime(data['Data Leitura'], format='%d/%m/%Y')

# Remover colunas não necessárias
data = data.drop(columns=['Bacia Estudo'])

# data = data.drop(columns=[
#     'Pressao Atmosferica Max',
#     'Pressao Atmosferica Min',
#     'Temperatura Bulbo',
#     'Umidade Relativa Ar',
#     'Umidade Max Hora Ant',
#     'Umidade Min Hora Ant'
# ])

# Seleção de características e alvo
features = ['Precipitacao', 'Pressao Atmosferica Estacao', 'Pressao Atmosferica Max',
            'Pressao Atmosferica Min', 'Radiacao Global', 'Temperatura Bulbo',
            'Temperatura Ponto Orvalho', 'Temperatura Max', 'Temperatura Min',
            'Temperatura Orvalho Max', 'Temperatura Orvalho Min', 'Umidade Max Hora Ant',
            'Umidade Min Hora Ant', 'Umidade Relativa Ar', 'Vento Direcao Horaria',
            'Vento Rajada', 'Vento Velocidade Horaria']

# features = ['Precipitacao', 'Pressao Atmosferica Estacao',
#             'Radiacao Global',
#             'Temperatura Ponto Orvalho', 'Temperatura Max', 'Temperatura Min',
#             'Temperatura Orvalho Max', 'Temperatura Orvalho Min',
#             'Vento Direcao Horaria',
#             'Vento Rajada', 'Vento Velocidade Horaria']

target = 'Vazao Observada'

# Normalizar os dados
scaler = MinMaxScaler()
data[features + [target]] = scaler.fit_transform(data[features + [target]])

# Preparar os dados para LSTM
sequence_length = 10  # Número de timesteps
X = []
y = []

for i in range(sequence_length, len(data)):
    X.append(data[features].iloc[i-sequence_length:i].values)
    y.append(data[target].iloc[i])

X, y = np.array(X), np.array(y)

# Divisão em treinamento, validação e teste
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=seed_value)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=seed_value)

# # Função para Augmentar os Dados (Exemplo: adicionando ruído gaussiano) - duplicando os dados
# def augment_data(X, y, noise_factor=0.01):
#     """
#     Função para aumentar dados tabulares adicionando ruído gaussiano.
#     """
#     X_augmented = X + noise_factor * np.random.normal(size=X.shape)
#     y_augmented = y + noise_factor * np.random.normal(size=y.shape)
#     return X_augmented, y_augmented

#Triplicando os dados
# Função para Augmentar os Dados (Exemplo: adicionando ruído gaussiano)
def augment_data(X, y, noise_factor=0.01):
    """
    Função para aumentar dados tabulares adicionando ruído gaussiano.
    """
    X_augmented_1 = X + noise_factor * np.random.normal(size=X.shape)  # Primeira versão aumentada
    y_augmented_1 = y + noise_factor * np.random.normal(size=y.shape)

    X_augmented_2 = X + noise_factor * np.random.normal(size=X.shape)  # Segunda versão aumentada
    y_augmented_2 = y + noise_factor * np.random.normal(size=y.shape)

    return X_augmented_1, y_augmented_1, X_augmented_2, y_augmented_2

# # Aumentando os dados de treinamento
# X_train_augmented, y_train_augmented = augment_data(X_train, y_train, noise_factor=0.01)

# Aumentando os dados de treinamento (triplicando)
X_train_augmented_1, y_train_augmented_1, X_train_augmented_2, y_train_augmented_2 = augment_data(X_train, y_train, noise_factor=0.01)

# Concatenando dados originais e aumentados
X_train_total = np.concatenate([X_train, X_train_augmented_1, X_train_augmented_2], axis=0)
y_train_total = np.concatenate([y_train, y_train_augmented_1, y_train_augmented_2], axis=0)

# Verificar quantidade de dados antes do Augmented
print(f"Total de dados de treinamento antes do Augmented Data: {X_train.shape[0]} amostras.")
print(f"Total de dados após aumento: {X_train_total.shape[0]} amostras de treinamento.")

# Construção do modelo LSTM com camada adicional
model = Sequential([
    LSTM(64, activation='tanh', return_sequences=True, input_shape=(sequence_length, len(features))),
    LSTM(64, activation='tanh', return_sequences=True),  # Nova camada LSTM
    LSTM(64, activation='tanh'),  # Camada LSTM adicional
    Dense(1)  # Saída única para prever a vazão
])

# Configuração da taxa de aprendizado
learning_rate = 0.001

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse', metrics=['mae'])

# # Caminho para salvar o modelo
# checkpoint_path = '/content/drive/MyDrive/Unicamp/Doutorado/TeseFinal_Dados/LSTM_CheckPoint_epoch_{epoch:02d}.weights.h5'
# checkpoint_callback = ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, save_best_only=False)

# # Treinamento
# history = model.fit(
#     X_train_augmented, y_train_augmented,  # Usar dados aumentados
#     validation_data=(X_val, y_val),
#     epochs=200,
#     batch_size=32
# )
# Treinamento
history = model.fit(
    X_train_total, y_train_total,
    validation_data=(X_val, y_val),
    epochs=200,
    batch_size=32,
    shuffle=False
)

# Salvar previsões
y_pred = model.predict(X_test)

# Inverter a escala dos dados para interpretação
scaler_alvo = MinMaxScaler()
scaler_alvo.min_, scaler_alvo.scale_ = scaler.min_[-1], scaler.scale_[-1]
y_test_original = scaler_alvo.inverse_transform(y_test.reshape(-1, 1)).flatten()
y_pred_original = scaler_alvo.inverse_transform(y_pred).flatten()

# Exportar os resultados
# Filtrar as datas correspondentes ao conjunto de teste
test_indices = data.index[-len(y_test):]  # Índices das últimas linhas usadas para o conjunto de teste
data_test = data.loc[test_indices, 'Data Leitura']

results = pd.DataFrame({
    'Data': data_test.values,
    'Valor_Observado': y_test_original,
    'Valor_Calculado': y_pred_original,
    'Erro': y_test_original - y_pred_original
})

results.to_csv('/content/drive/MyDrive/Unicamp/Doutorado/TeseFinal_Dados/IBITINGA INVERNADA/IbitingaInvernada_Resultados_LSTM(ApenasDados).csv', index=False, sep = ';')

# Calcular métricas
mae = mean_absolute_error(y_test_original, y_pred_original)
mse = mean_squared_error(y_test_original, y_pred_original)
rmse = np.sqrt(mse)
r2 = r2_score(y_test_original, y_pred_original)

# Calcular "acurácia" baseada em margem de tolerância
tolerance = 0.10  # 10% de tolerância
within_tolerance = np.abs(y_test_original - y_pred_original) <= (tolerance * y_test_original)
accuracy = np.mean(within_tolerance) * 100

# Imprimir métricas
print("Métricas do Modelo:")
print(f"MAE (Erro Médio Absoluto): {mae:.2f}")
print(f"MSE (Erro Quadrático Médio): {mse:.2f}")
print(f"RMSE (Raiz do Erro Quadrático Médio): {rmse:.2f}")
print(f"R² (Coeficiente de Determinação): {r2:.2f}")
print(f"Acurácia dentro de {tolerance * 100:.0f}% de tolerância: {accuracy:.2f}%")

# Análise de quais variáveis preditoras estão ajudando o modelo e quais não estão ajudando muito
from sklearn.inspection import permutation_importance
from sklearn.metrics import mean_squared_error

def custom_permutation_importance(model, X, y, baseline_score_func, scoring='neg_mean_squared_error'):
    """
    Função personalizada para calcular a importância das variáveis com dados 3D.
    """
    baseline_score = baseline_score_func(y, model.predict(X))  # Score inicial
    feature_importances = []

    for feature_idx in range(X.shape[2]):  # Iterar sobre as variáveis preditoras
        X_permuted = X.copy()

        # Permutar valores ao longo de todos os timesteps
        for timestep in range(X.shape[1]):
            np.random.shuffle(X_permuted[:, timestep, feature_idx])

        permuted_score = baseline_score_func(y, model.predict(X_permuted))
        feature_importances.append(baseline_score - permuted_score)

    return np.array(feature_importances)

importance_scores = custom_permutation_importance(
    model,
    X_test,
    y_test,
    baseline_score_func=lambda y_true, y_pred: -mean_squared_error(y_true, y_pred)
)

# Mostrar os resultados
for idx, score in enumerate(importance_scores):
    print(f"Importância da variável {features[idx]}: {score:.4f}")

#Analisando os resultados de importância das variáveis, as colunas com valores de importância extremamente baixos (próximos a zero)
#provavelmente não estão contribuindo significativamente para a previsão da vazão e poderiam ser removidas.
#As seguintes variáveis têm importâncias muito pequenas:

#Pressao Atmosferica Max: 0.0000
#Pressao Atmosferica Min: -0.0000
#Temperatura Bulbo: 0.0004
#Umidade Relativa Ar: 0.0001
#Umidade Max Hora Ant: 0.0005
#Umidade Min Hora Ant: 0.0006
#Essas variáveis poderiam ser consideradas para remoção, pois suas importâncias são muito baixas. No entanto, antes de removê-las,
#é interessante avaliar o desempenho do modelo com e sem essas colunas para garantir que a remoção não afete a performance de forma inesperada.