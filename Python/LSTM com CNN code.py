# -*- coding: utf-8 -*-
"""IbitingaGaviaoPeixoto-128 pixels - TeseFinalDoutorado(LSTM e CNN).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16KYOwZcRzOUxj0h6c3NEp5eD72WhvvQl
"""

pip install matplotlib

!pip install tqdm

import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt
import random
from datetime import datetime
from PIL import Image, UnidentifiedImageError
import os
import numpy as np
from tqdm import tqdm
import pandas as pd
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint

from google.colab import drive
drive.mount('/content/drive')

import glob
checkpoint_path = "/content/drive/MyDrive/Unicamp/Doutorado/TeseFinal_Dados/CheckPoint_epoch_*.weights.h5"
checkpoints = glob.glob(checkpoint_path)

if checkpoints:
    # Encontra o arquivo mais recente com base na data de modificação
    latest_checkpoint = max(checkpoints, key=os.path.getctime)
    print(f"Último checkpoint encontrado: {latest_checkpoint}")  # Imprime o último checkpoint

    # Extraindo o número da época do nome do arquivo
    try:
        # A época é extraída do nome do arquivo usando a parte após '_epoch_'
        epoch_str = latest_checkpoint.split("_epoch_")[1].split(".weights.h5")[0]
        initial_epoch = int(epoch_str)
        print(f"Última época carregada: {initial_epoch}")  # Imprime a última época carregada
    except (IndexError, ValueError) as e:
        print(f"Erro ao extrair o número da época: {e}. Definindo initial_epoch como 0.")
        initial_epoch = 0
else:
    initial_epoch = 0  # Caso não haja checkpoints, começa na época 0


# checkpoint_path = "/content/drive/MyDrive/Unicamp/Doutorado/TeseFinal_Dados/CheckPoint_epoch_*.weights.h5"
# checkpoints = glob.glob(checkpoint_path)  # Certifique-se de que 'checkpoint_path' está correto
# if checkpoints:
#     # Encontra o arquivo mais recente pelo tempo de criação/modificação
#     latest_checkpoint = max(checkpoints, key=os.path.getctime)
#     print(f"Carregando o checkpoint: {latest_checkpoint}")

# Callback para salvar o modelo
checkpoint_path_callback = "/content/drive/MyDrive/Unicamp/Doutorado/TeseFinal_Dados/CheckPoint_epoch_{epoch:02d}.weights.h5"
checkpoint_callback = ModelCheckpoint(
    filepath=checkpoint_path,  # Nome do arquivo
    save_weights_only=True,    # Salva apenas os pesos
    save_best_only=False,      # Se True, salva apenas o melhor modelo
    monitor='val_loss',        # Monitorar a métrica 'val_loss'
    mode='min',                # Salvar se 'val_loss' diminuir
    verbose=1                  # Mostra mensagens no console
)

# Seed para reprodutibilidade
seed = 42
tf.random.set_seed(seed)
np.random.seed(seed)
random.seed(seed)

# Carregando dados do CSV
data = pd.read_csv('/content/drive/MyDrive/Unicamp/Doutorado/TeseFinal_Dados/IBITINGA GAVIAO PEIXOTO/LSTM e CNN/IBITINGA GAVIAO PEIXOTO - Dados para processar rede-CSV.csv', delimiter=';')  # Ajuste o delimitador conforme o arquivo
data['Data Leitura'] = pd.to_datetime(data['Data Leitura'], format='%d/%m/%Y')

# Remover colunas não necessárias
data = data.drop(columns=['Bacia Estudo'])

# Selecionar colunas para o modelo
features = ['Precipitacao', 'Pressao Atmosferica Estacao', 'Pressao Atmosferica Max', 'Pressao Atmosferica Min',
            'Radiacao Global', 'Temperatura Bulbo', 'Temperatura Ponto Orvalho', 'Temperatura Max', 'Temperatura Min',
            'Temperatura Orvalho Max', 'Temperatura Orvalho Min', 'Umidade Max Hora Ant', 'Umidade Min Hora Ant',
            'Umidade Relativa Ar', 'Vento Direcao Horaria', 'Vento Rajada', 'Vento Velocidade Horaria']
target = 'Vazao Observada'

# Dividir dados em treino e teste, mas mantendo a coluna de Data Leitura
X_train, X_test, y_train, y_test = train_test_split(data[features + ['Data Leitura']], data[target], test_size=0.2, random_state=seed)

# Definir train_dates e test_dates a partir da coluna 'Data Leitura'
train_dates = X_train['Data Leitura'].dt.strftime('%d-%m-%Y').tolist()  # Extrai as datas formatadas de X_train
test_dates = X_test['Data Leitura'].dt.strftime('%d-%m-%Y').tolist()    # Extrai as datas formatadas de X_test

# Remover a coluna 'Data Leitura' de X_train e X_test, pois não será usada como entrada do modelo
X_train = X_train.drop(columns=['Data Leitura'])
X_test = X_test.drop(columns=['Data Leitura'])

def load_images_for_date(date):
    # Carrega imagens correspondentes a cada horário
    image_paths = [
         f"/content/drive/MyDrive/Unicamp/Doutorado/TeseFinal_Dados/GOES_Script e Imagens/IMG_2022_2023_2024 - 256 pixels/{date}_10.jpeg",
         f"/content/drive/MyDrive/Unicamp/Doutorado/TeseFinal_Dados/GOES_Script e Imagens/IMG_2022_2023_2024 - 256 pixels/{date}_12.jpeg",
         f"/content/drive/MyDrive/Unicamp/Doutorado/TeseFinal_Dados/GOES_Script e Imagens/IMG_2022_2023_2024 - 256 pixels/{date}_14.jpeg",
         f"/content/drive/MyDrive/Unicamp/Doutorado/TeseFinal_Dados/GOES_Script e Imagens/IMG_2022_2023_2024 - 256 pixels/{date}_16.jpeg"
    ]
    images = []
    for path in image_paths:
        try:
            with Image.open(path) as img:
              #Resize the image from 256 pixels to 128 pixels
              #images.append(np.array(img.resize((256, 256))))  # Ajusta a imagem de 256x256 pra 128x128
              #images.append(np.array(img.resize((128, 128)))/255)  # Ajusta a imagem de 256x256 pra 128x128 e normalizada
              images.append(np.array(img.resize((128, 128))))  # Ajusta a imagem de 256x256 pra 128x128 sem normalizacao
        except (FileNotFoundError, UnidentifiedImageError) as e:
            print(f"Aviso: Não foi possível carregar a imagem {path}. Erro: {e}")
            continue
    # Confere se todas as 4 imagens foram carregadas
    if len(images) != 4:
        print(f"Erro: Número de imagens para {date} não é 4.")
    return np.array(images)

# Modelo CNN para processar imagens
cnn_model = Sequential([
    #Conv2D(32, (3,3), activation='relu', input_shape=(256, 256, 3)),
    # MaxPooling2D((2,2)),
    # Conv2D(64, (3,3), activation='relu'),
    # MaxPooling2D((2,2)),
    # Conv2D(128, (3,3), activation='relu'),
    # MaxPooling2D((2,2)),
    # Flatten(),
    # Dense(64, activation='relu')

    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),
    MaxPooling2D((2, 2)),  # (64, 64, 32)
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),  # (32, 32, 64)
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),  # (16, 16, 128)
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),  # (8, 8, 128)
    Flatten(),  # Resultado final: 8192 elementos
    Dense(64, activation='relu')
])

import tensorflow.keras.backend as K

# Definição da função personalizada para RMSE
def root_mean_squared_error(y_true, y_pred):
    return K.sqrt(K.mean(K.square(y_pred - y_true)))

# Modelo LSTM para dados tabulares e combinação com CNN
def create_combined_model():
    # Definir a taxa de aprendizado desejada
    learning_rate = 0.003  # Substitua pelo valor que você deseja

    # Atualizar o otimizador com a taxa de aprendizado especificada
    MyOptimizer = Adam(learning_rate=learning_rate)

    #cnn_input = tf.keras.Input(shape=(4, 256, 256, 3))  # Para 4 imagens por dia
    cnn_input = tf.keras.Input(shape=(4, 128, 128, 3))  # Para 4 imagens por dia
    cnn_features = tf.keras.layers.TimeDistributed(cnn_model)(cnn_input)
    cnn_features = LSTM(64)(cnn_features)

    # Entrada para dados tabulares
    lstm_input = tf.keras.Input(shape=(len(features),))
    dense_features = Dense(64, activation='relu')(lstm_input)

    # Combinar CNN e LSTM
    combined = tf.keras.layers.concatenate([cnn_features, dense_features])
    output = Dense(1, activation='linear')(combined)

    model = tf.keras.Model(inputs=[cnn_input, lstm_input], outputs=output)

    # Compilar o modelo com o otimizador atualizado
    model.compile(
    optimizer=MyOptimizer,
    loss='mean_squared_error',  # Substitua pela função de perda que você está utilizando
    metrics=['mae', 'mse', root_mean_squared_error]  # Três métricas: MAE, MSE e RMSE
)
    return model

model = create_combined_model()

"""3. Treinamento e Avaliação python

"""

# Preparação dos dados de imagem e tabulares
def prepare_data(dataframe, target_column, all_dates):
    X_tabular, X_images, y = [], [], []
    for i, date in enumerate(tqdm(all_dates, desc="Preparando dados")):  # Adiciona a barra de progresso
        # Converter a string da data para datetime
        #DataInvertida = datetime.strptime(date, "%d-%m-%Y")  # Aqui convertemos para datetime no formato DD-MM-YYYY
        # Supondo que DataInvertida já está definida
        #print("Conteúdo de Data original:", date)
        # Aguarda o usuário pressionar "Enter" para continuar
        #input("Pressione Enter para continuar...")

        img_data = load_images_for_date(date)

        # Verifique se foram carregadas exatamente 4 imagens
        if img_data.shape[0] == 4:  # Verifica se existem 4 imagens
            X_images.append(img_data)
            X_tabular.append(dataframe.iloc[i].values)  # Adiciona as características (linha do DataFrame)
            y.append(target_column.iloc[i])  # Adiciona a vazão observada (target)
        else:
            print(f"Erro: Número de imagens para {date} não é 4.")

    return [np.array(X_images), np.array(X_tabular)], np.array(y)

# Preparar os dados, passando as variáveis corretamente
X_train_data, y_train_data = prepare_data(X_train, y_train, train_dates)
X_test_data, y_test_data = prepare_data(X_test, y_test, test_dates)

#model.load_weights(latest_checkpoint)

#print(latest_checkpoint)
#print(initial_epoch)

inicio_processo = datetime.now()

# Treinamento
# history = model.fit(
#     X_train_data, y_train_data, epochs=1, validation_data=(X_test_data, y_test_data), callbacks=[checkpoint_callback]
#     )
history = model.fit(
    X_train_data,
    y_train_data,
    epochs=100,
    validation_data=(X_test_data, y_test_data),
    batch_size=64,  # Alterado de 32 para 64
    initial_epoch=initial_epoch,  # Retoma a partir da última época
    callbacks=[checkpoint_callback]
)
fim_processo = datetime.now()

Tempo_execucao_minutos = (fim_processo - inicio_processo).total_seconds() /60

print(f"tempo de execução: {Tempo_execucao_minutos:.2f} minutos.")

"""4. Geração de Gráficos e Relatório"""

# Previsão e avaliação
y_pred = model.predict(X_test_data)

# Cálculo de métricas
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)  # Calculando o RMSE a partir do MSE

print(f"Mean Absolute Error: {mae}")
print(f"Mean Squared Error: {mse}")
print(f"Root Mean Squared Error (RMSE): {rmse}")

# Gráfico de comparação
plt.figure(figsize=(10,5))
plt.plot(y_test, label="Observado")
plt.plot(y_pred, label="Calculado")
plt.xlabel("Dia")
plt.ylabel("Vazão")
plt.legend()
plt.show()

# Converter X_test_data para um array NumPy, caso ainda seja uma lista
X_test_array = np.array(X_test_data[1])  # Acessando os dados tabulares (segundo elemento da lista)

# Certifique-se de que a dimensão de X_test_array seja apropriada
if X_test_array.ndim == 2:  # Verificar se é bidimensional
    data_leitura = X_test_array[:, 0]  # Supondo que a primeira coluna contém 'Data Leitura'
else:
    raise ValueError("Os dados tabulares de X_test_data não possuem formato esperado.")

# Garantir que y_pred e y_test_data estão no formato correto
y_pred = y_pred.flatten()  # Converte para 1D
y_test_values = y_test_data.flatten()  # Converte para 1D

# Filtrar as datas correspondentes ao conjunto de teste
test_indices = data.index[-len(y_test):]  # Índices das últimas linhas usadas para o conjunto de teste
data_test = data.loc[test_indices, 'Data Leitura']

# Criação do DataFrame para o relatório
results = pd.DataFrame({
    'Data_Leitura': data_test,  # A primeira coluna de X_test_array usada como Data Leitura
    'Valor_Calculado': y_pred,
    'Valor_Observado': y_test_values,
    'Erro': y_test_values - y_pred
})

# Obter a data atual no formato DD_MM_YYYY
hoje = datetime.now().strftime('%d_%m_%Y')

# Definir o nome do arquivo
nome_arquivo = f"ResultadosIbitingaGaviaoPeixoto_LSTM_CNN_{hoje}.csv"

# Caminho completo no Google Drive
output_path = f'/content/drive/MyDrive/Unicamp/Doutorado/TeseFinal_Dados/IBITINGA GAVIAO PEIXOTO/LSTM e CNN/{nome_arquivo}'


# Certifique-se de que a pasta existe antes de salvar
import os
os.makedirs(os.path.dirname(output_path), exist_ok=True)

# Salve o arquivo CSV no caminho especificado
results.to_csv(output_path, index=False, sep=';')

print(f"Arquivo salvo em: {output_path}")